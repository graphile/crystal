import LRU from "@graphile/lru";
import chalk from "chalk";
import debugFactory from "debug";
import type {
  Deferred,
  ExecutableStep,
  ExecutionEventEmitter,
  GrafastValuesList,
  ObjectStep,
  PromiseOrDirect,
} from "grafast";
import { defer, exportAs, isAsyncIterable, isDev } from "grafast";
import type { SQLRawValue } from "pg-sql2";

import { formatSQLForDebugging } from "./formatSQLForDebugging.js";
import { inspect } from "./inspect.js";

const LOOK_DOWN = "ðŸ‘‡".repeat(30);
const LOOK_UP = "ðŸ‘†".repeat(30);

const $$FINISHED: unique symbol = Symbol("finished");

class Wrapped<T extends Error | typeof $$FINISHED = Error | typeof $$FINISHED> {
  constructor(public originalValue: T) {}
}

let cursorCount = 0;

const debug = debugFactory("@dataplan/pg:PgExecutor");
const debugVerbose = debug.extend("verbose");
const debugExplain = debug.extend("explain");

type PublishFunction = (
  text: string,
  name: string | undefined,
  explain: string | undefined,
) => void;

type ExecuteFunction = <TData>(
  text: string,
  values: ReadonlyArray<SQLRawValue>,
) => Promise<PgClientResult<TData>>;

export interface PgClientQuery {
  /** The query string */
  text: string;
  /** The values to put in the placeholders */
  values?: Array<any>;
  /** An optimisation, to avoid you having to decode attribute names */
  arrayMode?: boolean;
  /** For prepared statements */
  name?: string;
}

export interface PgClientResult<TData> {
  /**
   * For `SELECT` or `INSERT/UPDATE/DELETE ... RETURNING` this will be the list
   * of rows returned.
   */
  rows: readonly TData[];
  /**
   * For `INSERT/UPDATE/DELETE` without `RETURNING`, this will be the number of
   * rows created/updated/deleted.
   */
  rowCount: number;
}

/**
 * This is a generic interface that your Postgres client must honor; have a
 * look at adaptors/pg.ts for an example of a compliant implementation that
 * uses the `pg` module, but you should be able to write an adaptor for many
 * different Postgres drivers.
 */
export interface PgClient {
  query<TData>(opts: PgClientQuery): Promise<PgClientResult<TData>>;
  withTransaction<T>(callback: (client: this) => Promise<T>): Promise<T>;
}

export interface WithPgClient<TPgClient extends PgClient = PgClient> {
  <T>(
    pgSettings: Record<string, string | undefined> | null,
    callback: (client: TPgClient) => T | Promise<T>,
  ): Promise<T>;

  release?(): PromiseOrDirect<void>;
}

export type PgExecutorContext<
  TSettings = any,
  TPgClient extends PgClient = PgClient,
> = {
  pgSettings: TSettings;
  withPgClient: WithPgClient<TPgClient>;
};

export type PgExecutorContextPlans<
  TSettings = any,
  TPgClient extends PgClient = PgClient,
> = {
  pgSettings: ExecutableStep<TSettings>;
  withPgClient: ExecutableStep<WithPgClient<TPgClient>>;
};

export type PgExecutorInput<TInput> = {
  context: PgExecutorContext;
  queryValues: ReadonlyArray<TInput>;
};

export type PgExecutorOptions = {
  text: string;
  rawSqlValues: Array<SQLRawValue>;
  identifierIndex?: number | null;
  name?: string;
  eventEmitter: ExecutionEventEmitter | undefined;
  useTransaction?: boolean;
};

export type PgExecutorMutationOptions = {
  context: PgExecutorContext;
  text: string;
  values: ReadonlyArray<SQLRawValue>;
};

export type PgExecutorSubscribeOptions = {
  context: PgExecutorContext;
  topic: string;
};

/**
 * Represents a PostgreSQL database connection, can be used for issuing queries
 * to the database. Used by PgResource but also directly by things like
 * PgSimpleFunctionCallStep. Was once PgDataSource itself. Multiple PgExecutors
 * can exist in the same schema. PgExecutor is also responsible for things like
 * caching.
 */
export class PgExecutor<const TName extends string = string, TSettings = any> {
  public name: TName;
  private contextCallback: () => ObjectStep<PgExecutorContextPlans<TSettings>>;
  private $$cache: symbol;

  constructor(options: {
    name: TName;
    context: () => ObjectStep<PgExecutorContextPlans<TSettings>>;
  }) {
    const { name, context } = options;
    this.name = name;
    this.$$cache = Symbol(this.name + "_cache");
    this.contextCallback = context;
  }

  public toString(): string {
    return chalk.bold.blue(`PgExecutor(${this.name})`);
  }

  // public context(): ExecutableStep
  public context(): ObjectStep<PgExecutorContextPlans<TSettings>> {
    return this.contextCallback();
  }

  private async _executeWithClient<TData>(
    client: PgClient,
    text: string,
    values: ReadonlyArray<SQLRawValue>,
    name?: string,
    publish?: PublishFunction,
  ): Promise<PgClientResult<TData>> {
    let queryResult: PgClientResult<TData> | null = null,
      error: any = null;
    const start = process.hrtime.bigint();
    try {
      queryResult = await client.query({
        text,
        values: values as SQLRawValue[],
        arrayMode: true,
        name,
      });
    } catch (e) {
      error = e;
    }
    const end = process.hrtime.bigint();
    // TODO: this should be based on the headers of the incoming request
    const shouldExplain = debugExplain.enabled;
    const explainAnalyzeSafe = shouldExplain && /^\s*select/i.test(text);
    let explain: string | undefined = undefined;
    if (shouldExplain && !error) {
      const explainResult = await client.query<{ 0: string }>({
        text: `EXPLAIN (${
          explainAnalyzeSafe ? "ANALYZE, " : ""
        }COSTS, VERBOSE, BUFFERS, SETTINGS) ${text}`,
        values: values as SQLRawValue[],
        arrayMode: true,
      });
      const firstResult = explainResult.rows[0];
      const key = firstResult ? (Object.keys(firstResult)[0] as "0") : "0";
      explain =
        explainResult.rows.length === 1 &&
        typeof firstResult[key] === "object" &&
        firstResult[key] !== null
          ? // Support for 'FORMAT JSON'
            JSON.stringify(firstResult[key], null, 2)
          : explainResult.rows.map((r) => r[key]).join("\n");
    }
    if (debugVerbose.enabled || debugExplain.enabled) {
      const duration = (Number((end - start) / 10000n) / 100).toFixed(2) + "ms";
      const rows = queryResult?.rows;
      const rowResults =
        rows && rows.length > 10
          ? "[\n  " +
            rows
              .slice(0, 3)
              .map((row) =>
                inspect(row, { colors: true, depth: 5 }).replace(/\n/g, "\n  "),
              )
              .join(",\n  ") +
            ",\n\n  ...\n\n  " +
            rows
              .slice(rows.length - 3)
              .map((row) =>
                inspect(row, { colors: true, depth: 5 }).replace(/\n/g, "\n  "),
              )
              .join("\n  ") +
            "\n]"
          : inspect(queryResult?.rows, { colors: true, depth: 6 });
      (debugExplain.enabled ? debugExplain : debugVerbose)(
        `\


%s
# SQL QUERY:
%s

# PLACEHOLDERS:
%o

${
  error
    ? `\
# ERROR:
%o`
    : `\
# RESULT:
%s`
}

# DURATION
${duration}
%s


# EXPLAIN
%s


`,
        LOOK_DOWN,
        formatSQLForDebugging(text, error),
        values,
        error ? error : rowResults,
        LOOK_UP,
        explain ??
          (shouldExplain
            ? `(Explain disabled ${
                error ? "due to error" : "due to unknown reason"
              })`
            : `(Use 'DEBUG="@dataplan/pg:PgExecutor:explain"' to enable explain)`),
      );
    }
    if (publish !== undefined) {
      publish(text, name, explain);
    }
    if (error) {
      throw error;
    }
    if (!queryResult) {
      // Appease TypeScript
      throw new Error("No query result and no error? Impossible.");
    }
    return queryResult;
  }

  private async _execute<TData>(
    context: PgExecutorContext,
    text: string,
    values: ReadonlyArray<SQLRawValue>,
    name?: string,
    publish?: PublishFunction,
  ) {
    // PERF: we could probably make this more efficient by grouping the
    // deferreds further, DataLoader-style, and running one SQL query for
    // everything.
    return await context.withPgClient(context.pgSettings, (client) =>
      this._executeWithClient<TData>(client, text, values, name, publish),
    );
  }

  private withTransaction<T>(
    context: PgExecutorContext,
    callback: (execute: ExecuteFunction) => Promise<T>,
  ): Promise<T> {
    return context.withPgClient<T>(context.pgSettings, (baseClient) =>
      baseClient.withTransaction((transactionClient) => {
        const execute: ExecuteFunction = (text, values) =>
          this._executeWithClient(transactionClient, text, values);
        return callback(execute);
      }),
    );
  }

  public async executeWithCache<TInput = any, TOutput = any>(
    values: GrafastValuesList<PgExecutorInput<TInput>>,
    common: PgExecutorOptions,
  ): Promise<{
    values: GrafastValuesList<ReadonlyArray<TOutput>>;
  }> {
    return this._executeWithOrWithoutCache<TInput, TOutput>(
      values,
      common,
      true,
    );
  }

  public async executeWithoutCache<TInput = any, TOutput = any>(
    values: GrafastValuesList<PgExecutorInput<TInput>>,
    common: PgExecutorOptions,
  ): Promise<{
    values: GrafastValuesList<ReadonlyArray<TOutput>>;
  }> {
    return this._executeWithOrWithoutCache<TInput, TOutput>(
      values,
      common,
      false,
    );
  }

  private async _executeWithOrWithoutCache<TInput = any, TOutput = any>(
    values: GrafastValuesList<PgExecutorInput<TInput>>,
    common: PgExecutorOptions,
    useCache: boolean,
  ): Promise<{
    values: GrafastValuesList<ReadonlyArray<TOutput>>;
  }> {
    const { rawSqlValues, identifierIndex, eventEmitter } = common;

    const publishExecute: PublishFunction | undefined = eventEmitter
      ? (text, name, explain) => {
          eventEmitter.emit("explainOperation", {
            operation: {
              type: "sql",
              title: `SQL query${name ? ` '${name.slice(0, 7)}...'` : ""}`,
              query: text,
              explain,
            },
          });
        }
      : undefined;

    const valuesCount = values.length;
    const results: Array<Deferred<Array<TOutput>> | undefined> = [];

    const batches = (() => {
      if (common.useTransaction) {
        // If we're using a transaction, use a separate batch for each query.
        return values.map(
          ({ context, queryValues }, resultIndex) =>
            [context, [{ queryValues, resultIndex }]] as const,
        );
      } else {
        // Group by context
        const groupMap = new Map<
          PgExecutorContext,
          Array<{
            queryValues: readonly any[];
            resultIndex: number;
          }>
        >();
        for (
          let resultIndex = 0, l = valuesCount;
          resultIndex < l;
          resultIndex++
        ) {
          results[resultIndex] = undefined;
          const { context, queryValues } = values[resultIndex];

          let entry = groupMap.get(context);
          if (!entry) {
            entry = [];
            groupMap.set(context, entry);
          }
          entry.push({ queryValues, resultIndex });
        }
        return groupMap.entries();
      }
    })();

    // For each context, run the relevant fetches
    const promises: Promise<void>[] = [];
    for (const [context, batch] of batches) {
      promises.push(
        (async () => {
          let cacheForContext = useCache
            ? (context as any)[this.$$cache]
            : null;
          if (!cacheForContext) {
            cacheForContext = new LRU({ maxLength: 500 /* SQL queries */ });
            if (useCache) {
              (context as any)[this.$$cache] = cacheForContext;
            }
          }

          const textAndValues = `${common.text}\n${JSON.stringify(
            rawSqlValues,
          )}`;
          let cacheForQuery = cacheForContext.get(textAndValues);
          if (!cacheForQuery) {
            cacheForQuery = new Map();
            cacheForContext.set(textAndValues, cacheForQuery);
          }

          const scopedCache = cacheForQuery;

          /**
           * The `identifiersJSON` (`JSON.stringify(queryValues)`) that don't exist in the cache currently.
           */
          const remaining: string[] = [];
          const remainingDeferreds: Array<Deferred<any[]>> = [];

          try {
            // Concurrent requests to the same queryValues should result in the same value/execution.
            const batchSize = batch.length;
            for (let batchIndex = 0; batchIndex < batchSize; batchIndex++) {
              const { queryValues, resultIndex } = batch[batchIndex];
              const identifiersJSON = JSON.stringify(queryValues); // PERF: Canonical? Manual for perf?
              const existingResult = scopedCache.get(identifiersJSON);
              if (existingResult) {
                if (debugVerbose.enabled) {
                  debugVerbose(
                    "%s served %o from cache: %c",
                    this,
                    identifiersJSON,
                    existingResult,
                  );
                }
                results[resultIndex] = existingResult;
              } else {
                if (debugVerbose.enabled) {
                  debugVerbose(
                    "%s no entry for %o in cache %c",
                    this,
                    identifiersJSON,
                    scopedCache,
                  );
                }
                if (isDev && remaining.includes(identifiersJSON)) {
                  throw new Error(
                    "Should only fetch each identifiersJSON once, future entries in the loop should receive previous deferred",
                  );
                }
                const pendingResult = defer<any[]>(); // CRITICAL: this MUST resolve later
                results[resultIndex] = pendingResult;
                scopedCache.set(identifiersJSON, pendingResult);
                remaining.push(identifiersJSON) - 1;
                remainingDeferreds.push(pendingResult);
              }
            }

            if (remaining.length) {
              const { text, name } = common;

              const sqlValues =
                identifierIndex == null
                  ? rawSqlValues
                  : [
                      ...rawSqlValues,
                      // Manual JSON-ing
                      "[" + remaining.join(",") + "]",
                    ];

              // PERF: we could probably make this more efficient by grouping the
              // deferreds further, DataLoader-style, and running one SQL query for
              // everything.
              const queryResult = common.useTransaction
                ? await this.executeMutation<TOutput>({
                    context,
                    text,
                    values: sqlValues,
                  })
                : await this._execute<TOutput>(
                    context,
                    text,
                    sqlValues,
                    name,
                    publishExecute,
                  );
              const { rows } = queryResult;
              const groups: { [valueIndex: number]: any[] } =
                Object.create(null);
              for (let i = 0, l = rows.length; i < l; i++) {
                const result = rows[i];
                const valueIndex =
                  identifierIndex != null
                    ? (result as number[])[identifierIndex]
                    : 0;
                if (!groups[valueIndex]) {
                  groups[valueIndex] = [result];
                } else {
                  groups[valueIndex].push(result);
                }
              }
              for (let i = 0, l = remainingDeferreds.length; i < l; i++) {
                const remainingDeferred = remainingDeferreds[i];
                const value = groups[i] ?? [];
                remainingDeferred.resolve(value);
              }
            }
          } catch (e) {
            // This block guarantees that all remainingDeferreds will be
            // rejected - we don't want defers hanging around!
            remainingDeferreds.forEach((d) => {
              try {
                d.reject(e);
              } catch (e2) {
                // Ignore error when rejecting
                console.error(
                  `Encountered second error when rejecting deferred due to a different error; ignoring error: ${e2}`,
                );
              }
            });
            return Promise.reject(e);
          }
        })(),
      );
    }

    // Avoids UnhandledPromiseRejection error.
    await Promise.allSettled(promises);

    const finalResults = await Promise.all(
      results as typeof results extends ReadonlyArray<infer U>
        ? ReadonlyArray<Exclude<U, undefined>>
        : never,
    );
    return { values: finalResults };
  }

  /**
   * Returns a list of streams (async iterables), one for each entry in
   * `values`, for the results from the cursor defined by running the query
   * `common.text` with the given variables.
   */
  public async executeStream<TInput = any, TOutput = any>(
    values: GrafastValuesList<PgExecutorInput<TInput>>,
    common: PgExecutorOptions,
  ): Promise<{
    streams: Array<AsyncIterable<TOutput> | PromiseLike<never> | null>;
  }> {
    const { text, rawSqlValues, identifierIndex } = common;

    const valuesCount = values.length;
    const streams: Array<AsyncIterable<TOutput> | Promise<never> | null> = [];

    // Group by context
    const groupMap = new Map<
      PgExecutorContext,
      Array<{
        queryValues: readonly any[];
        resultIndex: number;
      }>
    >();
    for (let resultIndex = 0, l = valuesCount; resultIndex < l; resultIndex++) {
      streams[resultIndex] = null;
      const { context, queryValues } = values[resultIndex];

      let entry = groupMap.get(context);
      if (!entry) {
        entry = [];
        groupMap.set(context, entry);
      }
      entry.push({ queryValues, resultIndex });
    }

    // For each context, run the relevant fetches
    const promises: Promise<void>[] = [];
    for (const [context, batch] of groupMap.entries()) {
      // ENHANCE: this is a mess, we should refactor and simplify it significantly
      const tx = defer();
      let txResolved = false;
      let cursorOpen = false;
      const promise = (async () => {
        const batchIndexesByIdentifiersJSON = new Map<string, number[]>();

        // Concurrent requests to the same queryValues should result in the same value/execution.
        const batchSize = batch.length;
        for (let batchIndex = 0; batchIndex < batchSize; batchIndex++) {
          const { queryValues } = batch[batchIndex];
          const identifiersJSON = JSON.stringify(queryValues); // Perf: Canonical? Manual for perf?
          const existing = batchIndexesByIdentifiersJSON.get(identifiersJSON);
          if (existing !== undefined) {
            existing.push(batchIndex);
            if (debugVerbose.enabled) {
              debugVerbose(
                "%s served %o again (%o)",
                this,
                identifiersJSON,
                existing,
              );
            }
            //results[resultIndex] = existingResult;
          } else {
            if (debugVerbose.enabled) {
              debugVerbose(
                "%s no entry for %o, allocating",
                this,
                identifiersJSON,
              );
            }
            batchIndexesByIdentifiersJSON.set(identifiersJSON, [batchIndex]);
          }
        }

        if (batchIndexesByIdentifiersJSON.size <= 0) {
          throw new Error(
            "GrafastInternalError<98699a62-cd44-4372-8e92-d730b116a51d>: empty batch doesn't make sense in this context.",
          );
        }

        const remaining = [...batchIndexesByIdentifiersJSON.keys()];
        const batchIndexesByValueIndex = [
          ...batchIndexesByIdentifiersJSON.values(),
        ];

        // PERF: batchIndexesByIdentifiersJSON = null;

        const sqlValues =
          identifierIndex == null
            ? rawSqlValues
            : [
                ...rawSqlValues,
                // Manual JSON-ing
                "[" + remaining.join(",") + "]",
              ];

        // Maximum PostgreSQL identifier length is typically 63 bytes.
        // Minus the `__cursor___` text, this leaves 52 characters for this
        // counter. JS's largest safe integer is 2^53-1 which is 16 digits
        // long - well under the 52 character limit. Assuming we used 1000
        // cursors per second every second, it would take us 285k years to
        // exhaust this. Because this is a cursor we control and know is
        // PostgreSQL safe we don't need to escape it.
        const cursorIdentifier = `__cursor_${cursorCount++}__`;

        const batchFetchSize = 100;

        const declareCursorSQL = `declare ${cursorIdentifier} insensitive no scroll cursor without hold for\n${text}`;
        const pullViaCursorSQL = `fetch forward ${batchFetchSize} from ${cursorIdentifier}`;
        const releaseCursorSQL = `close ${cursorIdentifier}`;

        let _deferredStreams = 0;
        let valuesPending = 0;

        const pending: Array<any[]> = batch.map(() => []);
        const waiting: Array<Deferred<any> | null> = batch.map(() => null);
        let finished = false;

        // eslint-disable-next-line no-inner-declarations
        function getNext(batchIndex: number): PromiseLike<any> {
          if (pending[batchIndex].length > 0) {
            const value = pending[batchIndex].shift();
            valuesPending--;
            if (valuesPending < batchFetchSize && !fetching) {
              fetchNextBatch().then(null, handleFetchError);
            }
            if (value instanceof Wrapped) {
              return Promise.reject(value.originalValue);
            } else {
              return value;
            }
          } else {
            if (finished) {
              throw $$FINISHED;
            }
            _deferredStreams++;
            if (isDev && waiting[batchIndex]) {
              throw new Error(`Waiting on more than one record! Forbidden!`);
            }
            const deferred = defer<any>();
            waiting[batchIndex] = deferred;
            return deferred;
          }
        }

        // eslint-disable-next-line no-inner-declarations
        function supplyValue(batchIndex: number, value: any | Wrapped): void {
          const deferred = waiting[batchIndex];
          if (deferred !== null) {
            waiting[batchIndex] = null;
            _deferredStreams--;
            if (value instanceof Wrapped) {
              deferred.reject(value.originalValue);
            } else {
              deferred.resolve(value);
            }
          } else {
            valuesPending++;
            pending[batchIndex].push(value);
          }
        }

        const executePromise = defer<ExecuteFunction>();
        const handleFetchError = (error: Error) => {
          if (finished) {
            console.error(
              `GraphileInternalError<2a6a34e4-a172-4c9a-b74e-b87ccf1b6d47>: Received an error when stream was already finished: ${error}`,
            );
            return;
          }
          finished = true;
          tx.resolve();
          txResolved = true;
          cursorOpen = false;
          executePromise.reject(error);
          console.error("Error occurred:");
          console.error(error);
          for (let i = 0, l = batch.length; i < l; i++) {
            supplyValue(i, new Wrapped(error));
          }
        };

        this.withTransaction(context, async (_execute) => {
          executePromise.resolve(_execute);
          return tx;
        }).then(null, handleFetchError);
        const execute = await executePromise;

        // eslint-disable-next-line no-inner-declarations
        let fetching = false;
        const fetchNextBatch = async (): Promise<void> => {
          if (fetching) {
            return;
          }
          if (finished) {
            return;
          }
          fetching = true;
          const queryResult = await execute<TOutput>(pullViaCursorSQL, []);
          const { rows } = queryResult;
          if (rows.length < batchFetchSize) {
            releaseCursor();
          }
          for (let i = 0, l = rows.length; i < l; i++) {
            const result = rows[i];
            const valueIndex =
              identifierIndex != null
                ? (result as number[])[identifierIndex]
                : 0;
            const batchIndexes = batchIndexesByValueIndex[valueIndex];
            if (!batchIndexes) {
              throw new Error(
                `GrafastInternalError<8f513ceb-a3dc-4ec7-9ca1-0f0d4576a22d>: could not determine the identifier JSON for value index '${valueIndex}'`,
              );
            }
            for (let i = 0, l = batchIndexes.length; i < l; i++) {
              supplyValue(batchIndexes[i], result);
            }
          }
          fetching = false;
          if (finished) {
            // We've hit the end of the road
            for (let i = 0, l = batch.length; i < l; i++) {
              supplyValue(i, new Wrapped($$FINISHED));
            }
          } else {
            if (valuesPending < batchFetchSize) {
              fetchNextBatch().then(null, handleFetchError);
            }
          }
        };

        // Registers the cursor
        cursorOpen = true;
        await execute<TOutput>(declareCursorSQL, sqlValues);

        // Ensure we release the cursor now we've registered it.
        fetchNextBatch().then(null, handleFetchError);
        function releaseCursor() {
          finished = true;
          if (cursorOpen) {
            cursorOpen = false;
            // Release the cursor
            (async () => {
              // This also closes the cursor
              try {
                await execute(releaseCursorSQL, []);
              } finally {
                if (!txResolved) {
                  tx.resolve();
                  txResolved = true;
                  cursorOpen = false;
                }
              }
            })().catch((e) => {
              console.error(`Error occurred whilst closing cursor: ${e}`);
            });
          }
        }
        // IMPORTANT: must *NOT* throw between here and the try block in the callback below
        let remainingBatches = batch.length;
        batch.forEach(({ resultIndex }, batchIndex) => {
          streams[resultIndex] = (async function* () {
            try {
              for (;;) {
                yield await getNext(batchIndex);
              }
            } catch (e) {
              if (e === $$FINISHED) {
                return;
              } else {
                throw e;
              }
            } finally {
              remainingBatches--;
              if (remainingBatches === 0) {
                releaseCursor();
              }
            }
          })();
        });
      })();
      promise.then(null, (e) => {
        console.error("UNEXPECTED ERROR!");
        console.error(e);
        tx.resolve();
        txResolved = true;
        cursorOpen = false;
        batch.forEach(({ resultIndex }) => {
          const stream = streams[resultIndex];
          if (isAsyncIterable(stream)) {
            stream[Symbol.asyncIterator]().throw?.(e);
          }
          streams[resultIndex] = Promise.reject(e);
        });
      });
      promises.push(promise);
    }

    // Avoids UnhandledPromiseRejection error.
    await Promise.allSettled(promises);

    return {
      streams: streams as Array<AsyncIterable<TOutput> | PromiseLike<never>>,
    };
  }

  public async executeMutation<TData>(
    options: PgExecutorMutationOptions,
  ): Promise<PgClientResult<TData>> {
    const { context, text, values } = options;
    const { withPgClient, pgSettings } = context;

    // We don't explicitly need a transaction for mutations
    const queryResult = await withPgClient(pgSettings, (client) =>
      this._executeWithClient<TData>(client, text, values),
    );

    // PERF: we could probably make this more efficient rather than blowing away the entire cache!
    // Wipe the cache since a mutation succeeded.
    (context as any)[this.$$cache]?.reset();

    return queryResult;
  }
}

exportAs("@dataplan/pg", PgExecutor, "PgExecutor");
